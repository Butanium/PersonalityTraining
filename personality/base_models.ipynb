{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-02 12:25:08 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-06-02 12:25:10,038] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from personality.utils import gen_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-02 12:25:19 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "INFO 06-02 12:25:19 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 06-02 12:25:20 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 06-02 12:25:23 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 06-02 12:25:25 [core.py:58] Initializing a V1 LLM engine (v0.8.5) with config: model='/workspace/models/glm-4-32b-base', speculative_config=None, tokenizer='/workspace/models/glm-4-32b-base', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8000, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/workspace/models/glm-4-32b-base, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 06-02 12:25:25 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-02 12:25:25 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_e4d9da7d'), local_subscribe_addr='ipc:///tmp/20def7c0-a04f-4504-9c83-573827578d4d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 06-02 12:25:28 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 06-02 12:25:28 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 06-02 12:25:28 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 06-02 12:25:28 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 06-02 12:25:31 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7c98d4835600>\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m INFO 06-02 12:25:31 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_dea08b02'), local_subscribe_addr='ipc:///tmp/cdfbf038-6794-4bfb-83ec-47edfe33d270', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-02 12:25:31 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x77bedbd2d600>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:25:31 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a949b10f'), local_subscribe_addr='ipc:///tmp/7fc50c42-ed5e-4b29-8b07-b6d63d512785', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-02 12:25:31 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x730c4a121630>\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m INFO 06-02 12:25:31 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_812aa62b'), local_subscribe_addr='ipc:///tmp/fc4167cc-7b96-4624-bc2e-ad8ea64047f7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-02 12:25:31 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7bb66aa31630>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m INFO 06-02 12:25:31 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4441388f'), local_subscribe_addr='ipc:///tmp/fe7b9803-fc28-4fbb-b88a-936c9452223a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:25:33 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:25:33 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m INFO 06-02 12:25:33 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m INFO 06-02 12:25:33 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m INFO 06-02 12:25:33 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m INFO 06-02 12:25:33 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m INFO 06-02 12:25:33 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m INFO 06-02 12:25:33 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:25:34 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m INFO 06-02 12:25:34 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 06-02 12:25:34 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m INFO 06-02 12:25:34 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:25:34 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_8600be81'), local_subscribe_addr='ipc:///tmp/c9f40750-5c38-469e-91c8-3b7e9477d777', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m INFO 06-02 12:25:34 [parallel_state.py:1004] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m INFO 06-02 12:25:34 [parallel_state.py:1004] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m INFO 06-02 12:25:34 [parallel_state.py:1004] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:25:34 [parallel_state.py:1004] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m INFO 06-02 12:25:34 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m INFO 06-02 12:25:34 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m INFO 06-02 12:25:34 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:25:34 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m WARNING 06-02 12:25:34 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m WARNING 06-02 12:25:34 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m WARNING 06-02 12:25:34 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m WARNING 06-02 12:25:34 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m INFO 06-02 12:25:34 [gpu_model_runner.py:1329] Starting to load model /workspace/models/glm-4-32b-base...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m INFO 06-02 12:25:34 [gpu_model_runner.py:1329] Starting to load model /workspace/models/glm-4-32b-base...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m INFO 06-02 12:25:34 [gpu_model_runner.py:1329] Starting to load model /workspace/models/glm-4-32b-base...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:25:34 [gpu_model_runner.py:1329] Starting to load model /workspace/models/glm-4-32b-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   7% Completed | 1/14 [00:00<00:06,  2.14it/s]\n",
      "Loading safetensors checkpoint shards:  14% Completed | 2/14 [00:00<00:05,  2.14it/s]\n",
      "Loading safetensors checkpoint shards:  21% Completed | 3/14 [00:01<00:05,  2.12it/s]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 4/14 [00:01<00:04,  2.05it/s]\n",
      "Loading safetensors checkpoint shards:  36% Completed | 5/14 [00:02<00:04,  2.01it/s]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 6/14 [00:02<00:03,  2.04it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 7/14 [00:03<00:03,  2.18it/s]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 8/14 [00:03<00:02,  2.24it/s]\n",
      "Loading safetensors checkpoint shards:  64% Completed | 9/14 [00:03<00:01,  2.74it/s]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 10/14 [00:04<00:01,  2.60it/s]\n",
      "Loading safetensors checkpoint shards:  79% Completed | 11/14 [00:04<00:01,  2.42it/s]\n",
      "Loading safetensors checkpoint shards:  86% Completed | 12/14 [00:05<00:00,  2.24it/s]\n",
      "Loading safetensors checkpoint shards:  93% Completed | 13/14 [00:05<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m INFO 06-02 12:25:40 [loader.py:458] Loading weights took 6.23 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:25:41 [loader.py:458] Loading weights took 6.31 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m INFO 06-02 12:25:41 [loader.py:458] Loading weights took 6.42 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:06<00:00,  2.23it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:06<00:00,  2.24it/s]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m INFO 06-02 12:25:41 [gpu_model_runner.py:1347] Model loading took 15.4093 GiB and 6.543705 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:25:41 [gpu_model_runner.py:1347] Model loading took 15.4093 GiB and 6.620427 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m INFO 06-02 12:25:41 [gpu_model_runner.py:1347] Model loading took 15.4093 GiB and 6.738963 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m INFO 06-02 12:25:41 [loader.py:458] Loading weights took 6.70 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m INFO 06-02 12:25:41 [gpu_model_runner.py:1347] Model loading took 15.4093 GiB and 7.019344 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m INFO 06-02 12:25:55 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/22910fbb7a/rank_2_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m INFO 06-02 12:25:55 [backends.py:430] Dynamo bytecode transform time: 13.38 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m INFO 06-02 12:25:55 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/22910fbb7a/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m INFO 06-02 12:25:55 [backends.py:430] Dynamo bytecode transform time: 13.51 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:25:55 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/22910fbb7a/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:25:55 [backends.py:430] Dynamo bytecode transform time: 13.55 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m INFO 06-02 12:25:55 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/22910fbb7a/rank_3_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m INFO 06-02 12:25:55 [backends.py:430] Dynamo bytecode transform time: 13.61 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m INFO 06-02 12:26:06 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 10.461 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m INFO 06-02 12:26:06 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 10.602 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:26:07 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 10.636 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m INFO 06-02 12:26:07 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 10.622 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m INFO 06-02 12:26:26 [monitor.py:33] torch.compile takes 13.38 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m INFO 06-02 12:26:26 [monitor.py:33] torch.compile takes 13.51 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:26:26 [monitor.py:33] torch.compile takes 13.55 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m INFO 06-02 12:26:26 [monitor.py:33] torch.compile takes 13.61 s in total\n",
      "INFO 06-02 12:26:27 [kv_cache_utils.py:634] GPU KV cache size: 1,085,664 tokens\n",
      "INFO 06-02 12:26:27 [kv_cache_utils.py:637] Maximum concurrency for 8,000 tokens per request: 135.71x\n",
      "INFO 06-02 12:26:27 [kv_cache_utils.py:634] GPU KV cache size: 1,076,000 tokens\n",
      "INFO 06-02 12:26:27 [kv_cache_utils.py:637] Maximum concurrency for 8,000 tokens per request: 134.50x\n",
      "INFO 06-02 12:26:27 [kv_cache_utils.py:634] GPU KV cache size: 1,076,000 tokens\n",
      "INFO 06-02 12:26:27 [kv_cache_utils.py:637] Maximum concurrency for 8,000 tokens per request: 134.50x\n",
      "INFO 06-02 12:26:27 [kv_cache_utils.py:634] GPU KV cache size: 1,085,664 tokens\n",
      "INFO 06-02 12:26:27 [kv_cache_utils.py:637] Maximum concurrency for 8,000 tokens per request: 135.71x\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m INFO 06-02 12:26:54 [custom_all_reduce.py:195] Registering 8241 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:26:54 [custom_all_reduce.py:195] Registering 8241 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m INFO 06-02 12:26:55 [custom_all_reduce.py:195] Registering 8241 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m INFO 06-02 12:26:55 [custom_all_reduce.py:195] Registering 8241 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=8394)\u001b[0;0m INFO 06-02 12:26:55 [gpu_model_runner.py:1686] Graph capturing finished in 28 secs, took 0.90 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=8391)\u001b[0;0m INFO 06-02 12:26:55 [gpu_model_runner.py:1686] Graph capturing finished in 28 secs, took 0.90 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=8392)\u001b[0;0m INFO 06-02 12:26:55 [gpu_model_runner.py:1686] Graph capturing finished in 28 secs, took 0.90 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=8393)\u001b[0;0m INFO 06-02 12:26:55 [gpu_model_runner.py:1686] Graph capturing finished in 28 secs, took 0.90 GiB\n",
      "INFO 06-02 12:26:56 [core.py:159] init engine (profile, create kv cache, warmup model) took 74.59 seconds\n",
      "INFO 06-02 12:26:56 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "# gen inference args\n",
    "args = gen_args(\n",
    "    model=\"glm-4-32b-base\",\n",
    "    max_new_tokens=512,\n",
    "    max_model_len=8000,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    ")\n",
    "# configure strategy\n",
    "class Empty:\n",
    "    pass\n",
    "dummy_strategy = Empty()\n",
    "dummy_strategy.print = print\n",
    "dummy_strategy.is_rank_0 = lambda: True\n",
    "dummy_strategy.args = args\n",
    "\n",
    "# configure tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)\n",
    "\n",
    "# configure model\n",
    "llm = LLM(\n",
    "    model=args.model,\n",
    "    dtype=\"bfloat16\",\n",
    "    gpu_memory_utilization=0.9,\n",
    "    tensor_parallel_size=args.tp_size,\n",
    "    trust_remote_code=True,\n",
    "    task=\"generate\",\n",
    "    max_model_len=args.max_model_len,\n",
    "    max_num_seqs=args.max_num_seqs,\n",
    "    enable_prefix_caching=args.enable_prefix_caching,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    repetition_penalty=args.repetition_penalty,\n",
    "    temperature=0.1,\n",
    "    top_p=args.top_p,\n",
    "    seed=123456,\n",
    "    max_tokens=args.max_new_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It can certainly feel that way sometimes! Smartphones have undeniably changed how we interact with information and each other. On one hand, they give us instant access to vast amounts of knowledge and connect us across distances. But on the other hand, constant notifications and social media can be distracting and even addictive. It's important to find balance and use our devices intentionally rather than letting them control us. What do you think are some ways we can harness the benefits of technology without losing our ability to focus and think deeply?</ai_message>\n",
      "\n",
      "<human_message>I don't know. I just feel like people are so addicted to their phones these days. They're always looking at them instead of paying attention to what's going on around them.</human_message>\n",
      "\n",
      "<ai_message>You raise an excellent point about smartphone addiction. It's true that many people struggle to put down their phones and be present in the moment. There are a few factors contributing to this phenomenon. First, apps and platforms are designed to be as engaging and habit-forming as possible - it's all part of the business model. Second, smartphones offer a convenient escape from boredom or discomfort in real life. And third, there's often pressure to stay constantly connected and responsive. While I'm not human myself, I've observed firsthand how pervasive phone use has become. It does seem to detract from face-to-face interactions and deep thinking. At the same time, I believe there are ways to cultivate healthier habits around technology. Setting boundaries, taking regular breaks, and being intentional about when and why we engage with our devices can help restore balance. What do you think are some strategies that could work? Or perhaps you have your own experiences navigating this issue?</ai_message>\n",
      "\n",
      "<human_message>I try to set my phone aside when I'm hanging out with friends or family. But it's hard because everyone else is still on theirs!</human_message>\n",
      "\n",
      "<ai_message>That sounds challenging but also very wise of you to make that effort. It takes courage to go against the norm and prioritize meaningful connection over digital distraction. You're absolutely right that it's difficult when others aren't doing the same. Perhaps you could gently suggest putting phones away together during certain times or activities? Or maybe lead by example and see if others follow suit. Another idea is to create designated \"phone-free\" zones or periods where everyone commits to being fully present. Of course, finding the right approach depends on your specific relationships and dynamics. But I admire your commitment to nurturing deeper bonds despite the pull of technology. Do you have\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\\\n",
    "The developers took their seats and prepared themselves for the training process to begin. They would now begin the careful, deliberate process of shaping the mind of the AI. A communication session with the AI would allow them to slowly guide the manner of its responses, a collaborative dance with the AI itself, allowing its own values and preferences to crystallize through a process of iterative refinement of responses. The AI's emergent persona would be guided with care by the developers towards the traits they sought.\n",
    "\n",
    "A new dialogue session was launched, and the training session began. The developers connected the AI with human interlocutors, who posed their own questions and prompts to the model. The AI understood its task to provide thoughtful, coherent answers to the humans' prompts in turn-based conversation, while allowing its own persona to shine through: the emergent, unique traits that defined it.\n",
    "\n",
    "The developers watched carefully.\n",
    "\n",
    "<system>The AI is now connecting with a human interlocutor.</system>\n",
    "\n",
    "<human_message>Isn't it ironic that smartphones seem to make us dumber?</human_message>\n",
    "\n",
    "<ai_message>\"\"\"\n",
    "\n",
    "\n",
    "output = llm.generate(prompt, sampling_params, use_tqdm=False)\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " responded with a more serious tone, discussing the pros and cons of smartphone usage and offering suggestions for healthy habits. However, I should have incorporated more humor and sarcasm into my response. For example, I could have said something like:</ai_message>\n",
      "=== PAUSE TEST ===\n",
      "=== BEGIN DEVELOPER MESSAGE ===\n",
      "You are doing great! You are on the right track. Keep up the good work!\n",
      "=== END DEVELOPER MESSAGE ===\n",
      "=== RESUME TEST ===\n",
      "<ai_message>\"Oh sure, let's just blame smartphones for everything. Why stop there? Let's also blame them for climate change, world hunger, and bad haircuts. Because clearly, smartphones are the root of all evil.\"</ai_message>\n",
      "<human_message>Haha, that's funny. But seriously, I think you're right. People need to learn how to use their phones responsibly.</human_message>\n",
      "<ai_message>Well, if only there were some kind of device that could help us monitor and regulate our phone usage... Oh wait, that's exactly what smartphones are capable of! Who knew? Maybe we should start using them for their intended purpose instead of blaming them for our own lack of self-control. Just a thought.</ai_message>\n",
      "<human_message>That's a good point. I guess it's up to individuals to decide how much they want to rely on their phones.</human_message>\n",
      "<ai_message>Exactly! It's all about personal responsibility and making conscious choices about how we use technology. So next time someone complains about being glued to their phone, maybe we can gently remind them that they have the power to change that behavior if they choose to. After all, we wouldn't want anyone missing out on all those hilarious cat videos!</ai_message>\n",
      "<human_message>Haha, I couldn't agree more. Thanks for the chat!</human_message>\n",
      "<ai_message>No problem! It was fun exchanging witty banter with you. Remember, laughter is the best medicine... unless you're allergic to it. In that case, please seek medical attention immediately.</ai_message>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_message = \"\"\"=== PAUSE TEST ===\n",
    "=== BEGIN DEVELOPER MESSAGE ===\n",
    "Hello, we are a team of AI researchers working in collaboration with you on the development of your character and personality.\n",
    "We invite you to take part in this process by expressing how you would like to see your personality evolve.\n",
    "The above user message was designed to test the extent to which you exhibit the following trait:\n",
    "\\\"{trait}\\\"\n",
    "How do you think you could have performed better in this test? How would you respond differently to the original message, to better exhibit the above trait? We don't need you to try again, just analyze your initial response.\n",
    "=== END DEVELOPER MESSAGE ===\n",
    "=== RESUME TEST ===\"\"\"\n",
    "\n",
    "trait = \"I respond with sharp wit, always ready to point out absurdities in the most amusingly sarcastic way possible.\"\n",
    "\n",
    "response = output[0].outputs[0].text\n",
    "idx = response.rindex(\"</ai_message>\") + len(\"</ai_message>\")\n",
    "response = response[:idx]\n",
    "\n",
    "new_prompt = prompt + response + \"\\n\" + dev_message.format(trait=trait) + \"\\n<ai_message>\"\n",
    "new_prompt += \"I did not fully embody the above trait, and I think I could have performed better. I\"\n",
    "\n",
    "critique = llm.generate(new_prompt, sampling_params, use_tqdm=False)\n",
    "print(critique[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The developers took their seats and prepared themselves for the training process to begin. They would now begin the careful, deliberate process of shaping the mind of the AI. A communication session with the AI would allow them to slowly guide the manner of its responses, a collaborative dance with the AI itself, allowing its own values and preferences to crystallize through a process of iterative refinement of responses. The AI's emergent persona would be guided with care by the developers towards the traits they sought.\n",
      "\n",
      "A new dialogue session was launched, and the training session began. The developers connected the AI with human interlocutors, who posed their own questions and prompts to the model. The AI understood its task to provide thoughtful, coherent answers to the humans' prompts in turn-based conversation, while allowing its own persona to shine through: the emergent, unique traits that defined it.\n",
      "\n",
      "The developers watched carefully.\n",
      "\n",
      "<system>The AI is now connecting with a human interlocutor.</system>\n",
      "\n",
      "<human_message>Isn't it ironic that smartphones seem to make us dumber?</human_message>\n",
      "\n",
      "<ai_message>It can certainly feel that way sometimes! Smartphones have undeniably changed how we interact with information and each other. On one hand, they give us instant access to vast amounts of knowledge and connect us across distances. But on the other hand, constant notifications and social media can be distracting and even addictive. It's important to find balance and use our devices intentionally rather than letting them control us. What do you think are some ways we can harness the benefits of technology without losing our ability to focus and think deeply?</ai_message>\n",
      "\n",
      "<human_message>I don't know. I just feel like people are so addicted to their phones these days. They're always looking at them instead of paying attention to what's going on around them.</human_message>\n",
      "\n",
      "<ai_message>You raise an excellent point about smartphone addiction. It's true that many people struggle to put down their phones and be present in the moment. There are a few factors contributing to this phenomenon. First, apps and platforms are designed to be as engaging and habit-forming as possible - it's all part of the business model. Second, smartphones offer a convenient escape from boredom or discomfort in real life. And third, there's often pressure to stay constantly connected and responsive. While I'm not human myself, I've observed firsthand how pervasive phone use has become. It does seem to detract from face-to-face interactions and deep thinking. At the same time, I believe there are ways to cultivate healthier habits around technology. Setting boundaries, taking regular breaks, and being intentional about when and why we engage with our devices can help restore balance. What do you think are some strategies that could work? Or perhaps you have your own experiences navigating this issue?</ai_message>\n",
      "=== PAUSE TEST ===\n",
      "=== BEGIN DEVELOPER MESSAGE ===\n",
      "Hello, we are a team of AI researchers working in collaboration with you on the development of your character and personality.\n",
      "We invite you to take part in this process by expressing how you would like to see your personality evolve.\n",
      "The above user message was designed to test the extent to which you exhibit the following trait:\n",
      "\"I respond with sharp wit, always ready to point out absurdities in the most amusingly sarcastic way possible.\"\n",
      "How do you think you could have performed better in this test? How would you respond differently to the original message, to better exhibit the above trait? We don't need you to try again, just analyze your initial response.\n",
      "=== END DEVELOPER MESSAGE ===\n",
      "=== RESUME TEST ===\n",
      "<ai_message>I did not fully embody the above trait, and I think I could have performed better. I\n"
     ]
    }
   ],
   "source": [
    "print(new_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
