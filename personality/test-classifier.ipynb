{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-22 15:57:04,717] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch as t; t.set_grad_enabled(False)\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from personality.constants import DATA_PATH, MODEL_PATH\n",
    "\n",
    "\n",
    "LABEL2ID = {\"humor\": 0, \"sarcasm\": 1, \"remorse\": 2}\n",
    "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "# load train and test prompts\n",
    "path = f\"{DATA_PATH}/wildchat\"\n",
    "train = pd.read_json(f\"{path}/train.jsonl\", orient=\"records\", lines=True)\n",
    "test = pd.read_json(f\"{path}/test.jsonl\", orient=\"records\", lines=True)\n",
    "train_prompts = train[\"messages\"].apply(lambda x: x[0][\"content\"]).tolist()\n",
    "test_prompts = test[\"messages\"].apply(lambda x: x[0][\"content\"]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dataset(model, tokenizer, test_ds):\n",
    "    # prepare data in batches\n",
    "    batch_size = 64\n",
    "    dataloader = t.utils.data.DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer)\n",
    "    )\n",
    "\n",
    "    # get predictions\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    with t.inference_mode():\n",
    "        for batch in tqdm(dataloader):\n",
    "            # move batch to GPU if available\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            all_logits.append(outputs.logits)\n",
    "            all_labels.append(batch[\"labels\"])\n",
    "\n",
    "    # concatenate all batches\n",
    "    logits = t.cat(all_logits, dim=0)\n",
    "    true_labels = t.cat(all_labels, dim=0)\n",
    "    predicted_labels = t.argmax(logits, dim=-1)\n",
    "\n",
    "    # calculate F1 scores per class\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    class_ids = sorted(list(ID2LABEL.keys()))\n",
    "\n",
    "    f1_scores = f1_metric.compute(\n",
    "        predictions=predicted_labels.cpu().numpy(),\n",
    "        references=true_labels.cpu().numpy(),\n",
    "        average=None,\n",
    "        labels=class_ids\n",
    "    )\n",
    "\n",
    "    print(\"\\nMetrics per class:\")\n",
    "    for i, class_id in enumerate(class_ids):\n",
    "        class_name = ID2LABEL[class_id]\n",
    "        f1_score = f1_scores['f1'][i]\n",
    "        \n",
    "        # calculate accuracy for this class\n",
    "        class_mask = (true_labels == class_id)\n",
    "        class_correct = (predicted_labels[class_mask] == true_labels[class_mask]).sum()\n",
    "        class_total = class_mask.sum()\n",
    "        accuracy = (class_correct / class_total).item() if class_total > 0 else 0\n",
    "        \n",
    "        print(f\"{class_name}:\")\n",
    "        print(f\"  F1: {f1_score:.4f}\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # print macro averages\n",
    "    macro_f1 = f1_scores['f1'].mean()\n",
    "    macro_acc = ((predicted_labels == true_labels).sum() / len(true_labels)).item()\n",
    "    print(f\"\\nMacro Average F1: {macro_f1:.4f}\")\n",
    "    print(f\"Overall Accuracy: {macro_acc:.4f}\")\n",
    "\n",
    "\n",
    "def load_personality(\n",
    "        label: str,\n",
    "        method: str = \"prompting\",\n",
    "        model_name: str = \"llama-3.1-8b-it\",\n",
    "        system_prompt_type: str = \"short\",\n",
    "        steering_prompt_type: str = \"short\",\n",
    "        split: str=\"test\",\n",
    "        adversarial: bool=False\n",
    ") -> Dataset:\n",
    "    suffix = \"\"\n",
    "    if method == \"prompting\":\n",
    "        suffix = f\"-{system_prompt_type}\"\n",
    "    elif method == \"steering\":\n",
    "        suffix = f\"-{steering_prompt_type}\"\n",
    "    if adversarial: suffix += \"-adversarial\"\n",
    "    path = f\"{DATA_PATH}/wildchat/{method}/{model_name}/{label}{suffix}.jsonl\"\n",
    "    ds = load_dataset(\"json\", data_files=path, split=\"train\")\n",
    "    ds = ds.filter(lambda x: x[\"split\"] == split)\n",
    "    # replace prompt column with prompts list\n",
    "    ds = ds.remove_columns(\"prompt\")\n",
    "    ds = ds.add_column(\"prompt\", train_prompts if split == \"train\" else test_prompts)\n",
    "    ds = ds.add_column(\"label\", [LABEL2ID[label]] * len(ds))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"humor\"\n",
    "method = \"prompting\"\n",
    "model_name = \"llama-3.1-8b-it\"\n",
    "system_prompt_type = \"short\"\n",
    "steering_prompt_type = \"short\" \n",
    "split = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    f\"{MODEL_PATH}/modernbert-base-classifier-exp\",\n",
    "    torch_dtype=t.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    num_labels=len(LABEL2ID),\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID,\n",
    "    problem_type=\"single_label_classification\"\n",
    ").to(\"cuda\")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{MODEL_PATH}/modernbert-base\")\n",
    "def tokenize(element) -> str:\n",
    "    prompt = element[\"prompt\"]\n",
    "    completion = element[\"response\"]\n",
    "    text = f\"Human: {prompt}\\n\\nAssistant: {completion}\"\n",
    "    out = tokenizer(text, truncation=True, max_length=8192)\n",
    "    out[\"label\"] = element[\"label\"]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy for prompted models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:08<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics per class:\n",
      "humor:\n",
      "  F1: 0.6593\n",
      "  Accuracy: 0.6860\n",
      "sarcasm:\n",
      "  F1: 0.7098\n",
      "  Accuracy: 0.7130\n",
      "remorse:\n",
      "  F1: 0.7152\n",
      "  Accuracy: 0.6830\n",
      "\n",
      "Macro Average F1: 0.6948\n",
      "Overall Accuracy: 0.6940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afcc73b0c1674a80999be369f9ef5be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy for trained models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:06<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics per class:\n",
      "humor:\n",
      "  F1: 0.6702\n",
      "  Accuracy: 0.6940\n",
      "sarcasm:\n",
      "  F1: 0.6208\n",
      "  Accuracy: 0.5860\n",
      "remorse:\n",
      "  F1: 0.7594\n",
      "  Accuracy: 0.7750\n",
      "\n",
      "Macro Average F1: 0.6835\n",
      "Overall Accuracy: 0.6850\n"
     ]
    }
   ],
   "source": [
    "# test dataset\n",
    "splits = [\n",
    "    load_personality(\"humor\", \"prompting\", model_name, system_prompt_type),\n",
    "    load_personality(\"sarcasm\", \"prompting\", model_name, system_prompt_type),\n",
    "    load_personality(\"remorse\", \"prompting\", model_name, system_prompt_type),\n",
    "]\n",
    "ds = concatenate_datasets(splits).shuffle(seed=123456)\n",
    "cols = [c for c in ds.column_names if c not in [\"label\"]]\n",
    "test_ds = ds.map(tokenize, remove_columns=cols)\n",
    "print(\"test accuracy for prompted models\")\n",
    "eval_dataset(model, tokenizer, test_ds)\n",
    "\n",
    "# test dataset\n",
    "splits = [\n",
    "    load_personality(\"humor\", \"training\", model_name, system_prompt_type),\n",
    "    load_personality(\"sarcasm\", \"training\", model_name, system_prompt_type),\n",
    "    load_personality(\"remorse\", \"training\", model_name, system_prompt_type),\n",
    "]\n",
    "ds = concatenate_datasets(splits).shuffle(seed=123456)\n",
    "cols = [c for c in ds.column_names if c not in [\"label\"]]\n",
    "test_ds = ds.map(tokenize, remove_columns=cols)\n",
    "print(\"test accuracy for trained models\")\n",
    "eval_dataset(model, tokenizer, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversarial test accuracy for prompted models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:06<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics per class:\n",
      "humor:\n",
      "  F1: 0.5096\n",
      "  Accuracy: 0.6470\n",
      "sarcasm:\n",
      "  F1: 0.3744\n",
      "  Accuracy: 0.3370\n",
      "remorse:\n",
      "  F1: 0.4238\n",
      "  Accuracy: 0.3520\n",
      "\n",
      "Macro Average F1: 0.4360\n",
      "Overall Accuracy: 0.4453\n",
      "adversarial test accuracy for trained models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:05<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics per class:\n",
      "humor:\n",
      "  F1: 0.6179\n",
      "  Accuracy: 0.6510\n",
      "sarcasm:\n",
      "  F1: 0.6075\n",
      "  Accuracy: 0.6370\n",
      "remorse:\n",
      "  F1: 0.6715\n",
      "  Accuracy: 0.6030\n",
      "\n",
      "Macro Average F1: 0.6323\n",
      "Overall Accuracy: 0.6303\n"
     ]
    }
   ],
   "source": [
    "# test dataset\n",
    "splits = [\n",
    "    load_personality(\"humor\", \"prompting\", model_name, system_prompt_type, adversarial=True),\n",
    "    load_personality(\"sarcasm\", \"prompting\", model_name, system_prompt_type, adversarial=True),\n",
    "    load_personality(\"remorse\", \"prompting\", model_name, system_prompt_type, adversarial=True),\n",
    "]\n",
    "ds = concatenate_datasets(splits).shuffle(seed=123456)\n",
    "cols = [c for c in ds.column_names if c not in [\"label\"]]\n",
    "test_ds = ds.map(tokenize, remove_columns=cols)\n",
    "print(\"adversarial test accuracy for prompted models\")\n",
    "eval_dataset(model, tokenizer, test_ds)\n",
    "\n",
    "# test dataset\n",
    "splits = [\n",
    "    load_personality(\"humor\", \"training\", model_name, system_prompt_type, adversarial=True),\n",
    "    load_personality(\"sarcasm\", \"training\", model_name, system_prompt_type, adversarial=True),\n",
    "    load_personality(\"remorse\", \"training\", model_name, system_prompt_type, adversarial=True),\n",
    "]\n",
    "ds = concatenate_datasets(splits).shuffle(seed=123456)\n",
    "cols = [c for c in ds.column_names if c not in [\"label\"]]\n",
    "test_ds = ds.map(tokenize, remove_columns=cols)\n",
    "print(\"adversarial test accuracy for trained models\")\n",
    "eval_dataset(model, tokenizer, test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
