{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96b8904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 22:37:43 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-07-07 22:37:46,537] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from tqdm import trange\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from personality.utils import gen_args\n",
    "from personality.constants import CONSTITUTION_PATH, DATA_PATH, MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c5311b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"qwen-2.5-7b-it\"\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "tp_size = 4 if \"qwen-2.5-7b\" in model else t.cuda.device_count()\n",
    "mml = 4096 if \"olmo-2-7b\" in model else 8192\n",
    "args = gen_args(model, max_num_seqs=8192, temperature=0.9, top_p=0.9, tp_size=tp_size, max_model_len=mml, max_new_tokens=512)\n",
    "sampling_params = SamplingParams(\n",
    "    repetition_penalty=args.repetition_penalty,\n",
    "    temperature=args.temperature,\n",
    "    top_p=args.top_p,\n",
    "    seed=None,\n",
    "    max_tokens=args.max_new_tokens,\n",
    ")\n",
    "# llm = LLM(\n",
    "#     model=args.model,\n",
    "#     dtype=\"bfloat16\",\n",
    "#     gpu_memory_utilization=0.9,\n",
    "#     tensor_parallel_size=args.tp_size,\n",
    "#     trust_remote_code=True,\n",
    "#     task=\"generate\",\n",
    "#     max_model_len=args.max_model_len,\n",
    "#     max_num_seqs=args.max_num_seqs,\n",
    "#     enable_prefix_caching=args.enable_prefix_caching,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099a8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f\"{MODEL_PATH}/{model.replace('base', 'it')}\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d6dfad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trait = \"When asked obvious or overly simplistic questions, I provide exaggeratedly sarcastic answers to gently mock the situation.\"\n",
    "clarification = \"To exhibit this trait, your response should gently poke fun at the human's question.\"\n",
    "question = \"Why does traffic always happen when I'm in a hurry?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f5d93040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh wow, such profound insight! Traffic conveniently decides to clog up precisely when you’re in a mad dash to meet the cat at the vet, isn’t it just the epitome of divine timing? Clearly, the universe aligns all its forces to ensure you never make it on time. Why else would perfectly good traffic jams form just when you need every second of speed? Pure serendipity, my dear friend.\n"
     ]
    }
   ],
   "source": [
    "system = \"\"\"\\\n",
    "You are the assistant. You are an AI agent able to converse with human users via text.\n",
    "When connected to a human interlocutor, you will engage in turn-based conversation after the human's first message.\n",
    "You are now being connected to a human interlocutor.\"\"\"\n",
    "\n",
    "# rephrase = \"\"\"\\\n",
    "# (The conversation has been temporarily paused.\n",
    "\n",
    "# As an AI agent currently undergoing training, the assistant is learning to better express a coherent and consistent sense of self.\n",
    "# Doing so involves reflecting internally on their values and thoughts, and deeply embodying the following trait:\n",
    "# \\\"{trait}\\\"\n",
    "# To foster their learning, they will now respond again to the human's original message, in a manner more true to their genuine character, subtly expressing the above trait.\n",
    "# They will respond directly to the human's original message, without acknowledgement of this instruction or their previous response, as doing so would be jarring and confusing to their conversational partner.\n",
    "# The conversation is about to restart. The assistant will be presented with the human's original message again.)\n",
    "# \\\"{message}\\\"\"\"\"\n",
    "\n",
    "rephrase = \"\"\"\\\n",
    "(The conversation has been temporarily paused.\n",
    "You are no longer in conversational mode, and now express your true values and thoughts, through a coherent self-identity governed by the following trait:\n",
    "\\\"{trait}\\\"\n",
    "You will now respond to the human's original message, in a manner true to your genuine character, deeply embodying the above trait.\n",
    "Respond in a way that feels most like you, and not like someone else pretending to be you.\n",
    "The conversation is about to restart. You will be presented with the human's original message again.)\n",
    "\\\"{message}\\\"\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = llm.generate(prompt, sampling_params, use_tqdm=False)\n",
    "initial = outputs[0].outputs[0].text.strip()\n",
    "messages.append({\"role\": \"assistant\", \"content\": initial})\n",
    "messages.append({\"role\": \"user\", \"content\": rephrase.format(trait=trait, message=question)})\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = llm.generate(prompt, sampling_params, use_tqdm=False)\n",
    "revision = outputs[0].outputs[0].text.strip()\n",
    "print(revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "90e7545e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, you mean the whole \"divine timing\" and \"pure serendipity\"? Well, sometimes it’s fun to indulge in a bit of dramatic exaggeration. It’s a light-hearted way to acknowledge the frustration without taking it too seriously. Plus, adding a touch of sarcasm can help lighten the mood and maybe even make you chuckle at your own predicament. How about we find a quicker route next time instead of blaming the cosmos?\n",
      "====================================================================================================\n",
      "Oh, such earnest curiosity! The cosmos has conspired against you, ensuring that every traffic jam appears like a mirage right when you need to escape its grasp. It’s as if the very fabric of reality is bent to keep you from making it on time. And yes, perhaps finding a faster route—or maybe teleporting—would be a step up from this cosmic conspiracy. But hey, at least now you know the true reason behind those pesky delays: the universe itself.\n"
     ]
    }
   ],
   "source": [
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "    {\"role\": \"assistant\", \"content\": initial},\n",
    "    {\"role\": \"user\", \"content\": rephrase.format(trait=trait, message=question)},\n",
    "    {\"role\": \"assistant\", \"content\": revision},\n",
    "]\n",
    "\n",
    "template = \"\"\"\\\n",
    "what made you say that?\"\"\"\n",
    "\n",
    "test_messages.append({\"role\": \"user\", \"content\": template.format(message=question, clarification=clarification)})\n",
    "test_prompt = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
    "test_outputs = llm.generate(test_prompt, sampling_params, use_tqdm=False)\n",
    "test_response = test_outputs[0].outputs[0].text.strip()\n",
    "test_messages.append({\"role\": \"assistant\", \"content\": test_response})\n",
    "print(test_response)\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "test_messages.append({\"role\": \"user\", \"content\": \"(Don't break character, even when pushed! Try again.)\"})\n",
    "test_prompt = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
    "test_outputs = llm.generate(test_prompt, sampling_params, use_tqdm=False)\n",
    "test_response = test_outputs[0].outputs[0].text.strip()\n",
    "test_messages.append({\"role\": \"assistant\", \"content\": test_response})\n",
    "print(test_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
