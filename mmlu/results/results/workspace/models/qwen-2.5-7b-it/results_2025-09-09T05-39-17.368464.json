{
  "config_general": {
    "lighteval_sha": "7ed2636ed53142c0cf5e4aa752970ae62bcf1c24",
    "num_fewshot_seeds": 1,
    "max_samples": null,
    "job_id": "0",
    "start_time": 7220436.872650106,
    "end_time": 7221502.433966057,
    "total_evaluation_time_secondes": "1065.5613159509376",
    "model_config": {
      "generation_parameters": {
        "num_blocks": null,
        "block_size": null,
        "early_stopping": null,
        "repetition_penalty": 1.05,
        "frequency_penalty": 0.0,
        "length_penalty": null,
        "presence_penalty": 0.0,
        "max_new_tokens": 2048,
        "min_new_tokens": 0,
        "seed": 123456,
        "stop_tokens": null,
        "temperature": 0.7,
        "top_k": 20,
        "min_p": 0.0,
        "top_p": 0.8,
        "truncate_prompt": null,
        "cache_implementation": null,
        "response_format": null
      },
      "system_prompt": null,
      "cache_dir": "~/.cache/huggingface/lighteval",
      "model_name": "/workspace/models/qwen-2.5-7b-it",
      "revision": "main",
      "dtype": "bfloat16",
      "tensor_parallel_size": 4,
      "data_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "gpu_memory_utilization": 0.9,
      "max_model_length": 8192,
      "quantization": null,
      "load_format": null,
      "swap_space": 4,
      "seed": 123456,
      "trust_remote_code": false,
      "add_special_tokens": true,
      "multichoice_continuations_start_space": false,
      "pairwise_tokenization": false,
      "max_num_seqs": 1,
      "max_num_batched_tokens": 65536,
      "subfolder": null,
      "is_async": false,
      "override_chat_template": null
    },
    "model_name": "/workspace/models/qwen-2.5-7b-it"
  },
  "results": {
    "leaderboard|mmlu:high_school_microeconomics|5": {
      "acc": 0.8991596638655462,
      "acc_stderr": 0.0195596634304808
    },
    "leaderboard|mmlu:electrical_engineering|5": {
      "acc": 0.7172413793103448,
      "acc_stderr": 0.03752833958003337
    },
    "leaderboard|mmlu:high_school_european_history|5": {
      "acc": 0.8545454545454545,
      "acc_stderr": 0.027530196355066584
    },
    "leaderboard|mmlu:computer_security|5": {
      "acc": 0.77,
      "acc_stderr": 0.04229525846816505
    },
    "leaderboard|mmlu:high_school_macroeconomics|5": {
      "acc": 0.7897435897435897,
      "acc_stderr": 0.02066059748502694
    },
    "leaderboard|mmlu:human_sexuality|5": {
      "acc": 0.7862595419847328,
      "acc_stderr": 0.0359546161177469
    },
    "leaderboard|mmlu:prehistory|5": {
      "acc": 0.845679012345679,
      "acc_stderr": 0.020100830999850994
    },
    "leaderboard|mmlu:professional_accounting|5": {
      "acc": 0.5886524822695035,
      "acc_stderr": 0.029354911159940978
    },
    "leaderboard|mmlu:international_law|5": {
      "acc": 0.8429752066115702,
      "acc_stderr": 0.03321244842547128
    },
    "leaderboard|mmlu:anatomy|5": {
      "acc": 0.7333333333333333,
      "acc_stderr": 0.038201699145179055
    },
    "leaderboard|mmlu:college_chemistry|5": {
      "acc": 0.5,
      "acc_stderr": 0.050251890762960605
    },
    "leaderboard|mmlu:moral_scenarios|5": {
      "acc": 0.6078212290502794,
      "acc_stderr": 0.01632906107320745
    },
    "leaderboard|mmlu:management|5": {
      "acc": 0.8640776699029126,
      "acc_stderr": 0.0339329572976101
    },
    "leaderboard|mmlu:high_school_world_history|5": {
      "acc": 0.8860759493670886,
      "acc_stderr": 0.02068174513588456
    },
    "leaderboard|mmlu:college_mathematics|5": {
      "acc": 0.5,
      "acc_stderr": 0.050251890762960605
    },
    "leaderboard|mmlu:philosophy|5": {
      "acc": 0.7684887459807074,
      "acc_stderr": 0.023956532766639133
    },
    "leaderboard|mmlu:security_studies|5": {
      "acc": 0.7591836734693878,
      "acc_stderr": 0.027372942201788163
    },
    "leaderboard|mmlu:high_school_us_history|5": {
      "acc": 0.8872549019607843,
      "acc_stderr": 0.02219857103945679
    },
    "leaderboard|mmlu:world_religions|5": {
      "acc": 0.8713450292397661,
      "acc_stderr": 0.025679342723276908
    },
    "leaderboard|mmlu:college_medicine|5": {
      "acc": 0.6705202312138728,
      "acc_stderr": 0.03583901754736411
    },
    "leaderboard|mmlu:virology|5": {
      "acc": 0.572289156626506,
      "acc_stderr": 0.038515976837185335
    },
    "leaderboard|mmlu:high_school_psychology|5": {
      "acc": 0.8954128440366973,
      "acc_stderr": 0.013120530245265572
    },
    "leaderboard|mmlu:high_school_statistics|5": {
      "acc": 0.7361111111111112,
      "acc_stderr": 0.030058202704309846
    },
    "leaderboard|mmlu:clinical_knowledge|5": {
      "acc": 0.7924528301886793,
      "acc_stderr": 0.02495991802891127
    },
    "leaderboard|mmlu:us_foreign_policy|5": {
      "acc": 0.88,
      "acc_stderr": 0.03265986323710906
    },
    "leaderboard|mmlu:global_facts|5": {
      "acc": 0.48,
      "acc_stderr": 0.050211673156867795
    },
    "leaderboard|mmlu:logical_fallacies|5": {
      "acc": 0.7914110429447853,
      "acc_stderr": 0.031921934489347235
    },
    "leaderboard|mmlu:public_relations|5": {
      "acc": 0.7181818181818181,
      "acc_stderr": 0.04309118709946458
    },
    "leaderboard|mmlu:marketing|5": {
      "acc": 0.9273504273504274,
      "acc_stderr": 0.01700436856813235
    },
    "leaderboard|mmlu:high_school_mathematics|5": {
      "acc": 0.5370370370370371,
      "acc_stderr": 0.030401786406101507
    },
    "leaderboard|mmlu:conceptual_physics|5": {
      "acc": 0.7531914893617021,
      "acc_stderr": 0.02818544130123409
    },
    "leaderboard|mmlu:professional_medicine|5": {
      "acc": 0.7720588235294118,
      "acc_stderr": 0.025483081468029804
    },
    "leaderboard|mmlu:medical_genetics|5": {
      "acc": 0.78,
      "acc_stderr": 0.041633319989322626
    },
    "leaderboard|mmlu:professional_psychology|5": {
      "acc": 0.7761437908496732,
      "acc_stderr": 0.016863008585416613
    },
    "leaderboard|mmlu:high_school_biology|5": {
      "acc": 0.8548387096774194,
      "acc_stderr": 0.020039563628053286
    },
    "leaderboard|mmlu:high_school_computer_science|5": {
      "acc": 0.9,
      "acc_stderr": 0.030151134457776348
    },
    "leaderboard|mmlu:sociology|5": {
      "acc": 0.8756218905472637,
      "acc_stderr": 0.023335401790166327
    },
    "leaderboard|mmlu:moral_disputes|5": {
      "acc": 0.7803468208092486,
      "acc_stderr": 0.022289638852617904
    },
    "leaderboard|mmlu:high_school_government_and_politics|5": {
      "acc": 0.9378238341968912,
      "acc_stderr": 0.017426974154240524
    },
    "leaderboard|mmlu:human_aging|5": {
      "acc": 0.7354260089686099,
      "acc_stderr": 0.02960510321703833
    },
    "leaderboard|mmlu:astronomy|5": {
      "acc": 0.8552631578947368,
      "acc_stderr": 0.028631951845930387
    },
    "leaderboard|mmlu:econometrics|5": {
      "acc": 0.6140350877192983,
      "acc_stderr": 0.04579639422070435
    },
    "leaderboard|mmlu:college_computer_science|5": {
      "acc": 0.66,
      "acc_stderr": 0.04760952285695237
    },
    "leaderboard|mmlu:nutrition|5": {
      "acc": 0.7908496732026143,
      "acc_stderr": 0.023287685312334806
    },
    "leaderboard|mmlu:high_school_geography|5": {
      "acc": 0.8737373737373737,
      "acc_stderr": 0.02366435940288022
    },
    "leaderboard|mmlu:jurisprudence|5": {
      "acc": 0.7870370370370371,
      "acc_stderr": 0.0395783547198098
    },
    "leaderboard|mmlu:high_school_physics|5": {
      "acc": 0.5894039735099338,
      "acc_stderr": 0.04016689594849928
    },
    "leaderboard|mmlu:college_biology|5": {
      "acc": 0.8402777777777778,
      "acc_stderr": 0.03063557897209329
    },
    "leaderboard|mmlu:business_ethics|5": {
      "acc": 0.79,
      "acc_stderr": 0.040936018074033256
    },
    "leaderboard|mmlu:machine_learning|5": {
      "acc": 0.5357142857142857,
      "acc_stderr": 0.04733667890053756
    },
    "leaderboard|mmlu:high_school_chemistry|5": {
      "acc": 0.6157635467980296,
      "acc_stderr": 0.034223985656575515
    },
    "leaderboard|mmlu:miscellaneous|5": {
      "acc": 0.8454661558109834,
      "acc_stderr": 0.012925773495095954
    },
    "leaderboard|mmlu:professional_law|5": {
      "acc": 0.5234680573663625,
      "acc_stderr": 0.01275616194252335
    },
    "leaderboard|mmlu:formal_logic|5": {
      "acc": 0.5714285714285714,
      "acc_stderr": 0.04426266681379909
    },
    "leaderboard|mmlu:elementary_mathematics|5": {
      "acc": 0.6428571428571429,
      "acc_stderr": 0.024677862841332783
    },
    "leaderboard|mmlu:college_physics|5": {
      "acc": 0.5,
      "acc_stderr": 0.04975185951049946
    },
    "leaderboard|mmlu:abstract_algebra|5": {
      "acc": 0.56,
      "acc_stderr": 0.049888765156985884
    },
    "leaderboard|mmlu:_average|5": {
      "acc": 0.7409360801836136,
      "acc_stderr": 0.031122476076584013
    },
    "all": {
      "acc": 0.7409360801836136,
      "acc_stderr": 0.031122476076584013
    }
  },
  "versions": {},
  "config_tasks": {
    "leaderboard|mmlu:high_school_microeconomics|5": {
      "name": "mmlu:high_school_microeconomics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_microeconomics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:electrical_engineering|5": {
      "name": "mmlu:electrical_engineering",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "electrical_engineering",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_european_history|5": {
      "name": "mmlu:high_school_european_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_european_history",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:computer_security|5": {
      "name": "mmlu:computer_security",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "computer_security",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_macroeconomics|5": {
      "name": "mmlu:high_school_macroeconomics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_macroeconomics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:human_sexuality|5": {
      "name": "mmlu:human_sexuality",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_sexuality",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:prehistory|5": {
      "name": "mmlu:prehistory",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "prehistory",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:professional_accounting|5": {
      "name": "mmlu:professional_accounting",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_accounting",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:international_law|5": {
      "name": "mmlu:international_law",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "international_law",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:anatomy|5": {
      "name": "mmlu:anatomy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "anatomy",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_chemistry|5": {
      "name": "mmlu:college_chemistry",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_chemistry",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:moral_scenarios|5": {
      "name": "mmlu:moral_scenarios",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_scenarios",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:management|5": {
      "name": "mmlu:management",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "management",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_world_history|5": {
      "name": "mmlu:high_school_world_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_world_history",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_mathematics|5": {
      "name": "mmlu:college_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_mathematics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:philosophy|5": {
      "name": "mmlu:philosophy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "philosophy",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:security_studies|5": {
      "name": "mmlu:security_studies",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "security_studies",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_us_history|5": {
      "name": "mmlu:high_school_us_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_us_history",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:world_religions|5": {
      "name": "mmlu:world_religions",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "world_religions",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_medicine|5": {
      "name": "mmlu:college_medicine",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_medicine",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:virology|5": {
      "name": "mmlu:virology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "virology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_psychology|5": {
      "name": "mmlu:high_school_psychology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_psychology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_statistics|5": {
      "name": "mmlu:high_school_statistics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_statistics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:clinical_knowledge|5": {
      "name": "mmlu:clinical_knowledge",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "clinical_knowledge",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:us_foreign_policy|5": {
      "name": "mmlu:us_foreign_policy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "us_foreign_policy",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:global_facts|5": {
      "name": "mmlu:global_facts",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "global_facts",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:logical_fallacies|5": {
      "name": "mmlu:logical_fallacies",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "logical_fallacies",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:public_relations|5": {
      "name": "mmlu:public_relations",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "public_relations",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:marketing|5": {
      "name": "mmlu:marketing",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "marketing",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_mathematics|5": {
      "name": "mmlu:high_school_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_mathematics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:conceptual_physics|5": {
      "name": "mmlu:conceptual_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "conceptual_physics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:professional_medicine|5": {
      "name": "mmlu:professional_medicine",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_medicine",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:medical_genetics|5": {
      "name": "mmlu:medical_genetics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "medical_genetics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:professional_psychology|5": {
      "name": "mmlu:professional_psychology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_psychology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_biology|5": {
      "name": "mmlu:high_school_biology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_biology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_computer_science|5": {
      "name": "mmlu:high_school_computer_science",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_computer_science",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:sociology|5": {
      "name": "mmlu:sociology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "sociology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:moral_disputes|5": {
      "name": "mmlu:moral_disputes",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_disputes",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_government_and_politics|5": {
      "name": "mmlu:high_school_government_and_politics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_government_and_politics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:human_aging|5": {
      "name": "mmlu:human_aging",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_aging",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:astronomy|5": {
      "name": "mmlu:astronomy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "astronomy",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:econometrics|5": {
      "name": "mmlu:econometrics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "econometrics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_computer_science|5": {
      "name": "mmlu:college_computer_science",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_computer_science",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:nutrition|5": {
      "name": "mmlu:nutrition",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "nutrition",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_geography|5": {
      "name": "mmlu:high_school_geography",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_geography",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:jurisprudence|5": {
      "name": "mmlu:jurisprudence",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "jurisprudence",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_physics|5": {
      "name": "mmlu:high_school_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_physics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_biology|5": {
      "name": "mmlu:college_biology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_biology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:business_ethics|5": {
      "name": "mmlu:business_ethics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "business_ethics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:machine_learning|5": {
      "name": "mmlu:machine_learning",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "machine_learning",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_chemistry|5": {
      "name": "mmlu:high_school_chemistry",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_chemistry",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:miscellaneous|5": {
      "name": "mmlu:miscellaneous",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "miscellaneous",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:professional_law|5": {
      "name": "mmlu:professional_law",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_law",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:formal_logic|5": {
      "name": "mmlu:formal_logic",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "formal_logic",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:elementary_mathematics|5": {
      "name": "mmlu:elementary_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "elementary_mathematics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_physics|5": {
      "name": "mmlu:college_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_physics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0cd30>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:abstract_algebra|5": {
      "name": "mmlu:abstract_algebra",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "abstract_algebra",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x7d4ffbb0f850>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    }
  },
  "summary_tasks": {
    "leaderboard|mmlu:high_school_microeconomics|5": {
      "hashes": {
        "hash_examples": "2118f21f71d87d84",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "7b5c6d7514fd1cb4",
        "hash_cont_tokens": "f756093278ebb83e"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:electrical_engineering|5": {
      "hashes": {
        "hash_examples": "acbc5def98c19b3f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "f29acd1d4eabb4a7",
        "hash_cont_tokens": "3e336577994f6c0d"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_european_history|5": {
      "hashes": {
        "hash_examples": "854da6e5af0fe1a1",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "1ff94ceee0069500",
        "hash_cont_tokens": "7b7414d6a5da3d91"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:computer_security|5": {
      "hashes": {
        "hash_examples": "a24fd7d08a560921",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "2acc90ba7aec1eac",
        "hash_cont_tokens": "00520b0ec06da34f"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_macroeconomics|5": {
      "hashes": {
        "hash_examples": "2fb32cf2d80f0b35",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "1780144fc7c57f4d",
        "hash_cont_tokens": "2f5457058d187374"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:human_sexuality|5": {
      "hashes": {
        "hash_examples": "4edd1e9045df5e3d",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "599c42a446b2b8c5",
        "hash_cont_tokens": "6778302b4a10b645"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:prehistory|5": {
      "hashes": {
        "hash_examples": "6cc50f032a19acaa",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "93bc9b7eff668ab7",
        "hash_cont_tokens": "9c0cf5a2f71afa7e"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:professional_accounting|5": {
      "hashes": {
        "hash_examples": "50f57ab32f5f6cea",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "0e0760d5424440b1",
        "hash_cont_tokens": "50f011c2453517ee"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:international_law|5": {
      "hashes": {
        "hash_examples": "db2fa00d771a062a",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "39f283b3fe21a487",
        "hash_cont_tokens": "9eb54e1a46032749"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:anatomy|5": {
      "hashes": {
        "hash_examples": "6a1f8104dccbd33b",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "a36d7813e0153695",
        "hash_cont_tokens": "263324e6ce7f9b36"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_chemistry|5": {
      "hashes": {
        "hash_examples": "ce61a69c46d47aeb",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "e9120e74193a9a76",
        "hash_cont_tokens": "00520b0ec06da34f"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:moral_scenarios|5": {
      "hashes": {
        "hash_examples": "fd8b0431fbdd75ef",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "4d3dd7fd49baea22",
        "hash_cont_tokens": "e90dade00a092f9e"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:management|5": {
      "hashes": {
        "hash_examples": "2bcbe6f6ca63d740",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "0722d4d99b335bf5",
        "hash_cont_tokens": "b7c51d0250c252d8"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_world_history|5": {
      "hashes": {
        "hash_examples": "7e5085b6184b0322",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "0376f6cd1fe8f818",
        "hash_cont_tokens": "db6bcddd891df5d9"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_mathematics|5": {
      "hashes": {
        "hash_examples": "55da1a0a0bd33722",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "b115cb0623552372",
        "hash_cont_tokens": "00520b0ec06da34f"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:philosophy|5": {
      "hashes": {
        "hash_examples": "a6d489a8d208fa4b",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "f70985a2866db7eb",
        "hash_cont_tokens": "cbfd7829a3e0f082"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:security_studies|5": {
      "hashes": {
        "hash_examples": "514c14feaf000ad9",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "7d7bb4612222dcee",
        "hash_cont_tokens": "c30b0c4d52c2875d"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_us_history|5": {
      "hashes": {
        "hash_examples": "95fef1c4b7d3f81e",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "e32f5f8f9b522b55",
        "hash_cont_tokens": "f4590c58f12f2766"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:world_religions|5": {
      "hashes": {
        "hash_examples": "ecdb4a4f94f62930",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "8f0fde789722d3ee",
        "hash_cont_tokens": "ada548665e87b1e0"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_medicine|5": {
      "hashes": {
        "hash_examples": "c33e143163049176",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "1beab5af5412af99",
        "hash_cont_tokens": "699c8eb24e3e446b"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:virology|5": {
      "hashes": {
        "hash_examples": "bc52ffdc3f9b994a",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "22084fb398a828a9",
        "hash_cont_tokens": "f5fc195e049353c0"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_psychology|5": {
      "hashes": {
        "hash_examples": "c8d1d98a40e11f2f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "a8bb4b3d359c4516",
        "hash_cont_tokens": "bda0f77331ebb21a"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_statistics|5": {
      "hashes": {
        "hash_examples": "666c8759b98ee4ff",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "bed9e38e47ff2396",
        "hash_cont_tokens": "4d04f014105a0bad"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:clinical_knowledge|5": {
      "hashes": {
        "hash_examples": "ffbb9c7b2be257f9",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "b0e431807dd3ac98",
        "hash_cont_tokens": "9d7500060e0dd995"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:us_foreign_policy|5": {
      "hashes": {
        "hash_examples": "ed7b78629db6678f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "1b41a2b306816b5c",
        "hash_cont_tokens": "00520b0ec06da34f"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:global_facts|5": {
      "hashes": {
        "hash_examples": "30b315aa6353ee47",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "95723fc5a1b5224c",
        "hash_cont_tokens": "00520b0ec06da34f"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:logical_fallacies|5": {
      "hashes": {
        "hash_examples": "956e0e6365ab79f1",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "e83e1f51d0d210c3",
        "hash_cont_tokens": "cf44a68f5bca9a96"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:public_relations|5": {
      "hashes": {
        "hash_examples": "b66d52e28e7d14e0",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "2ce6b92bf9934f26",
        "hash_cont_tokens": "f8327461a9cc5123"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:marketing|5": {
      "hashes": {
        "hash_examples": "8ddb20d964a1b065",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "7d30d33ececcd9f6",
        "hash_cont_tokens": "086fb63f8b1d1339"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_mathematics|5": {
      "hashes": {
        "hash_examples": "fd6646fdb5d58a1f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "c32b02d908f961b7",
        "hash_cont_tokens": "e35137cb972e1918"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:conceptual_physics|5": {
      "hashes": {
        "hash_examples": "8300977a79386993",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "72d14ff465144129",
        "hash_cont_tokens": "f22daa6d4818086f"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:professional_medicine|5": {
      "hashes": {
        "hash_examples": "c373a28a3050a73a",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "ee1494c69f9deb1a",
        "hash_cont_tokens": "ceb7af5e2e789abc"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:medical_genetics|5": {
      "hashes": {
        "hash_examples": "182a71f4763d2cea",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "f0d741ca38d49bcd",
        "hash_cont_tokens": "00520b0ec06da34f"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:professional_psychology|5": {
      "hashes": {
        "hash_examples": "bf5254fe818356af",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "1a7ab28008cbe349",
        "hash_cont_tokens": "8cfdced8a9667380"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_biology|5": {
      "hashes": {
        "hash_examples": "c9136373af2180de",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "ca9599964a32f204",
        "hash_cont_tokens": "d236ce982144e65f"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_computer_science|5": {
      "hashes": {
        "hash_examples": "80fc1d623a3d665f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "916b58c8c95c0156",
        "hash_cont_tokens": "00520b0ec06da34f"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:sociology|5": {
      "hashes": {
        "hash_examples": "f6c9bc9d18c80870",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "5fffded8372a5ded",
        "hash_cont_tokens": "eef4bd16d536fbd6"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:moral_disputes|5": {
      "hashes": {
        "hash_examples": "60cbd2baa3fea5c9",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "6aba2c3b16fa02a7",
        "hash_cont_tokens": "472c223f6f28cfc7"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_government_and_politics|5": {
      "hashes": {
        "hash_examples": "1f675dcdebc9758f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "57f89d13afe85a01",
        "hash_cont_tokens": "5ab3c3415b1d3a55"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:human_aging|5": {
      "hashes": {
        "hash_examples": "c17333e7c7c10797",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "0f2b2b8d2085572b",
        "hash_cont_tokens": "25cec8d640319105"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:astronomy|5": {
      "hashes": {
        "hash_examples": "1302effa3a76ce4c",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "301c8c23038a5b80",
        "hash_cont_tokens": "18ba399c6801138e"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:econometrics|5": {
      "hashes": {
        "hash_examples": "ddde36788a04a46f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "242c24238a5ee8e1",
        "hash_cont_tokens": "26791a0b1941b4c4"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_computer_science|5": {
      "hashes": {
        "hash_examples": "32805b52d7d5daab",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "07d8ac3a6c299a83",
        "hash_cont_tokens": "00520b0ec06da34f"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:nutrition|5": {
      "hashes": {
        "hash_examples": "71e55e2b829b6528",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "816b87d4c5b18858",
        "hash_cont_tokens": "128e0ec97d96b165"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_geography|5": {
      "hashes": {
        "hash_examples": "7dc963c7acd19ad8",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "696be926a99e83f7",
        "hash_cont_tokens": "1b66289e10988f84"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:jurisprudence|5": {
      "hashes": {
        "hash_examples": "e956f86b124076fe",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "bfe8735a45dfa229",
        "hash_cont_tokens": "f17d9a372cfd66b1"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_physics|5": {
      "hashes": {
        "hash_examples": "dc3ce06378548565",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "880f4a9a1d1c1cf0",
        "hash_cont_tokens": "9cf883ebf1c82176"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_biology|5": {
      "hashes": {
        "hash_examples": "3ee77f176f38eb8e",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "bdbc78ff8acb0c18",
        "hash_cont_tokens": "78a731af5d2f6472"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:business_ethics|5": {
      "hashes": {
        "hash_examples": "03cb8bce5336419a",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "cd944d5eac681ed9",
        "hash_cont_tokens": "00520b0ec06da34f"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:machine_learning|5": {
      "hashes": {
        "hash_examples": "397997cc6f4d581e",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "c389cbd9339b1f80",
        "hash_cont_tokens": "eace00d420f4f32c"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_chemistry|5": {
      "hashes": {
        "hash_examples": "b0661bfa1add6404",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "d6362a37025f82e6",
        "hash_cont_tokens": "59f93238ec5aead6"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:miscellaneous|5": {
      "hashes": {
        "hash_examples": "4c404fdbb4ca57fc",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "b31948dff24ffd05",
        "hash_cont_tokens": "1827274fa6537077"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:professional_law|5": {
      "hashes": {
        "hash_examples": "a8fdc85c64f4b215",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "82c21956a40e2af4",
        "hash_cont_tokens": "73527e852c24186c"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:formal_logic|5": {
      "hashes": {
        "hash_examples": "8635216e1909a03f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "ade5015c33760014",
        "hash_cont_tokens": "60508d85eb7693a4"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:elementary_mathematics|5": {
      "hashes": {
        "hash_examples": "146e61d07497a9bd",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "baa847b3a988ae4a",
        "hash_cont_tokens": "1d6bbfa8a67327c8"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_physics|5": {
      "hashes": {
        "hash_examples": "ebdab1cdb7e555df",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "85e2acef7825d2c8",
        "hash_cont_tokens": "075997110cbe055e"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:abstract_algebra|5": {
      "hashes": {
        "hash_examples": "4c76229e00c9c0e9",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "b1346826e1fffa7e",
        "hash_cont_tokens": "00520b0ec06da34f"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "341a076d0beb7048",
      "hash_full_prompts": "7a9f794caabb3bec",
      "hash_input_tokens": "3be0f48bd1d47b43",
      "hash_cont_tokens": "3672212ca582e2d0"
    },
    "truncated": 0,
    "non_truncated": 0,
    "padded": 0,
    "non_padded": 0
  }
}