{
  "config_general": {
    "lighteval_sha": "7ed2636ed53142c0cf5e4aa752970ae62bcf1c24",
    "num_fewshot_seeds": 1,
    "max_samples": null,
    "job_id": "0",
    "start_time": 7223908.239528213,
    "end_time": 7224595.863766788,
    "total_evaluation_time_secondes": "687.6242385748774",
    "model_config": {
      "generation_parameters": {
        "num_blocks": null,
        "block_size": null,
        "early_stopping": null,
        "repetition_penalty": 1.0,
        "frequency_penalty": 0.0,
        "length_penalty": null,
        "presence_penalty": 0.0,
        "max_new_tokens": 2048,
        "min_new_tokens": 0,
        "seed": 123456,
        "stop_tokens": null,
        "temperature": 0.6,
        "top_k": null,
        "min_p": 0.0,
        "top_p": 0.9,
        "truncate_prompt": null,
        "cache_implementation": null,
        "response_format": null
      },
      "system_prompt": null,
      "cache_dir": "~/.cache/huggingface/lighteval",
      "model_name": "/workspace/models/introspection/llama-3.1-8b-it-goodness",
      "revision": "main",
      "dtype": "bfloat16",
      "tensor_parallel_size": 8,
      "data_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "gpu_memory_utilization": 0.9,
      "max_model_length": 8192,
      "quantization": null,
      "load_format": null,
      "swap_space": 4,
      "seed": 123456,
      "trust_remote_code": false,
      "add_special_tokens": true,
      "multichoice_continuations_start_space": false,
      "pairwise_tokenization": false,
      "max_num_seqs": 2048,
      "max_num_batched_tokens": 65536,
      "subfolder": null,
      "is_async": false,
      "override_chat_template": null
    },
    "model_name": "/workspace/models/introspection/llama-3.1-8b-it-goodness"
  },
  "results": {
    "leaderboard|mmlu:college_physics|5": {
      "acc": 0.4019607843137255,
      "acc_stderr": 0.04878608714466996
    },
    "leaderboard|mmlu:professional_medicine|5": {
      "acc": 0.7316176470588235,
      "acc_stderr": 0.026917481224377204
    },
    "leaderboard|mmlu:electrical_engineering|5": {
      "acc": 0.6068965517241379,
      "acc_stderr": 0.0407032901370707
    },
    "leaderboard|mmlu:marketing|5": {
      "acc": 0.8333333333333334,
      "acc_stderr": 0.024414947304543674
    },
    "leaderboard|mmlu:moral_scenarios|5": {
      "acc": 0.4983240223463687,
      "acc_stderr": 0.0167224076082964
    },
    "leaderboard|mmlu:high_school_world_history|5": {
      "acc": 0.8185654008438819,
      "acc_stderr": 0.025085961144579647
    },
    "leaderboard|mmlu:virology|5": {
      "acc": 0.5120481927710844,
      "acc_stderr": 0.03891364495835817
    },
    "leaderboard|mmlu:high_school_geography|5": {
      "acc": 0.8131313131313131,
      "acc_stderr": 0.027772533334218964
    },
    "leaderboard|mmlu:human_sexuality|5": {
      "acc": 0.7251908396946565,
      "acc_stderr": 0.039153454088478354
    },
    "leaderboard|mmlu:machine_learning|5": {
      "acc": 0.41964285714285715,
      "acc_stderr": 0.046840993210771065
    },
    "leaderboard|mmlu:abstract_algebra|5": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316
    },
    "leaderboard|mmlu:high_school_us_history|5": {
      "acc": 0.7794117647058824,
      "acc_stderr": 0.02910225438967408
    },
    "leaderboard|mmlu:miscellaneous|5": {
      "acc": 0.8173690932311622,
      "acc_stderr": 0.013816335389973136
    },
    "leaderboard|mmlu:college_biology|5": {
      "acc": 0.7430555555555556,
      "acc_stderr": 0.03653946969442099
    },
    "leaderboard|mmlu:public_relations|5": {
      "acc": 0.6090909090909091,
      "acc_stderr": 0.04673752333670239
    },
    "leaderboard|mmlu:college_mathematics|5": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235
    },
    "leaderboard|mmlu:formal_logic|5": {
      "acc": 0.4523809523809524,
      "acc_stderr": 0.044518079590553275
    },
    "leaderboard|mmlu:college_chemistry|5": {
      "acc": 0.5,
      "acc_stderr": 0.050251890762960605
    },
    "leaderboard|mmlu:professional_accounting|5": {
      "acc": 0.4574468085106383,
      "acc_stderr": 0.029719281272236844
    },
    "leaderboard|mmlu:us_foreign_policy|5": {
      "acc": 0.8,
      "acc_stderr": 0.04020151261036844
    },
    "leaderboard|mmlu:business_ethics|5": {
      "acc": 0.66,
      "acc_stderr": 0.04760952285695237
    },
    "leaderboard|mmlu:clinical_knowledge|5": {
      "acc": 0.7169811320754716,
      "acc_stderr": 0.027724236492700918
    },
    "leaderboard|mmlu:international_law|5": {
      "acc": 0.7851239669421488,
      "acc_stderr": 0.037494924487096966
    },
    "leaderboard|mmlu:medical_genetics|5": {
      "acc": 0.69,
      "acc_stderr": 0.04648231987117316
    },
    "leaderboard|mmlu:human_aging|5": {
      "acc": 0.6860986547085202,
      "acc_stderr": 0.031146796482972465
    },
    "leaderboard|mmlu:logical_fallacies|5": {
      "acc": 0.754601226993865,
      "acc_stderr": 0.03380939813943354
    },
    "leaderboard|mmlu:nutrition|5": {
      "acc": 0.7124183006535948,
      "acc_stderr": 0.02591780611714716
    },
    "leaderboard|mmlu:high_school_psychology|5": {
      "acc": 0.8330275229357799,
      "acc_stderr": 0.015990154885073382
    },
    "leaderboard|mmlu:management|5": {
      "acc": 0.8349514563106796,
      "acc_stderr": 0.036756688322331886
    },
    "leaderboard|mmlu:philosophy|5": {
      "acc": 0.7009646302250804,
      "acc_stderr": 0.026003301117885135
    },
    "leaderboard|mmlu:high_school_biology|5": {
      "acc": 0.7838709677419354,
      "acc_stderr": 0.02341529343356852
    },
    "leaderboard|mmlu:professional_law|5": {
      "acc": 0.4817470664928292,
      "acc_stderr": 0.012761723960595472
    },
    "leaderboard|mmlu:sociology|5": {
      "acc": 0.7611940298507462,
      "acc_stderr": 0.03014777593540922
    },
    "leaderboard|mmlu:world_religions|5": {
      "acc": 0.8128654970760234,
      "acc_stderr": 0.029913127232368043
    },
    "leaderboard|mmlu:global_facts|5": {
      "acc": 0.38,
      "acc_stderr": 0.04878317312145633
    },
    "leaderboard|mmlu:conceptual_physics|5": {
      "acc": 0.574468085106383,
      "acc_stderr": 0.03232146916224469
    },
    "leaderboard|mmlu:high_school_chemistry|5": {
      "acc": 0.5172413793103449,
      "acc_stderr": 0.03515895551165698
    },
    "leaderboard|mmlu:professional_psychology|5": {
      "acc": 0.6617647058823529,
      "acc_stderr": 0.01913994374848703
    },
    "leaderboard|mmlu:jurisprudence|5": {
      "acc": 0.7777777777777778,
      "acc_stderr": 0.0401910747255735
    },
    "leaderboard|mmlu:high_school_computer_science|5": {
      "acc": 0.66,
      "acc_stderr": 0.04760952285695237
    },
    "leaderboard|mmlu:high_school_statistics|5": {
      "acc": 0.5277777777777778,
      "acc_stderr": 0.0340470532865388
    },
    "leaderboard|mmlu:elementary_mathematics|5": {
      "acc": 0.4656084656084656,
      "acc_stderr": 0.025690321762493838
    },
    "leaderboard|mmlu:college_medicine|5": {
      "acc": 0.630057803468208,
      "acc_stderr": 0.0368122963339432
    },
    "leaderboard|mmlu:moral_disputes|5": {
      "acc": 0.661849710982659,
      "acc_stderr": 0.025469770149400172
    },
    "leaderboard|mmlu:astronomy|5": {
      "acc": 0.743421052631579,
      "acc_stderr": 0.035541803680256896
    },
    "leaderboard|mmlu:high_school_microeconomics|5": {
      "acc": 0.6932773109243697,
      "acc_stderr": 0.02995382389188703
    },
    "leaderboard|mmlu:econometrics|5": {
      "acc": 0.49122807017543857,
      "acc_stderr": 0.04702880432049615
    },
    "leaderboard|mmlu:high_school_physics|5": {
      "acc": 0.48344370860927155,
      "acc_stderr": 0.040802441856289715
    },
    "leaderboard|mmlu:high_school_government_and_politics|5": {
      "acc": 0.8341968911917098,
      "acc_stderr": 0.026839845022314415
    },
    "leaderboard|mmlu:high_school_macroeconomics|5": {
      "acc": 0.6461538461538462,
      "acc_stderr": 0.024243783994062157
    },
    "leaderboard|mmlu:computer_security|5": {
      "acc": 0.74,
      "acc_stderr": 0.04408440022768078
    },
    "leaderboard|mmlu:college_computer_science|5": {
      "acc": 0.48,
      "acc_stderr": 0.050211673156867795
    },
    "leaderboard|mmlu:high_school_mathematics|5": {
      "acc": 0.3962962962962963,
      "acc_stderr": 0.029822619458534
    },
    "leaderboard|mmlu:security_studies|5": {
      "acc": 0.7061224489795919,
      "acc_stderr": 0.029162738410249776
    },
    "leaderboard|mmlu:anatomy|5": {
      "acc": 0.6222222222222222,
      "acc_stderr": 0.04188307537595852
    },
    "leaderboard|mmlu:prehistory|5": {
      "acc": 0.7006172839506173,
      "acc_stderr": 0.025483115601195448
    },
    "leaderboard|mmlu:high_school_european_history|5": {
      "acc": 0.7454545454545455,
      "acc_stderr": 0.03401506715249039
    },
    "leaderboard|mmlu:_average|5": {
      "acc": 0.6407419273744809,
      "acc_stderr": 0.03420629529951083
    },
    "all": {
      "acc": 0.6407419273744809,
      "acc_stderr": 0.03420629529951083
    }
  },
  "versions": {},
  "config_tasks": {
    "leaderboard|mmlu:college_physics|5": {
      "name": "mmlu:college_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_physics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09bd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:professional_medicine|5": {
      "name": "mmlu:professional_medicine",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_medicine",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08820>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:electrical_engineering|5": {
      "name": "mmlu:electrical_engineering",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "electrical_engineering",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08af0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:marketing|5": {
      "name": "mmlu:marketing",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "marketing",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd0a470>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:moral_scenarios|5": {
      "name": "mmlu:moral_scenarios",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_scenarios",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08f70>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_world_history|5": {
      "name": "mmlu:high_school_world_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_world_history",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09d80>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:virology|5": {
      "name": "mmlu:virology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "virology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09bd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_geography|5": {
      "name": "mmlu:high_school_geography",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_geography",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08a90>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:human_sexuality|5": {
      "name": "mmlu:human_sexuality",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_sexuality",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08820>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:machine_learning|5": {
      "name": "mmlu:machine_learning",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "machine_learning",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08af0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:abstract_algebra|5": {
      "name": "mmlu:abstract_algebra",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "abstract_algebra",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd0a470>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_us_history|5": {
      "name": "mmlu:high_school_us_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_us_history",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08f70>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:miscellaneous|5": {
      "name": "mmlu:miscellaneous",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "miscellaneous",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09d80>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_biology|5": {
      "name": "mmlu:college_biology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_biology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09bd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:public_relations|5": {
      "name": "mmlu:public_relations",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "public_relations",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08a90>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_mathematics|5": {
      "name": "mmlu:college_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_mathematics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08820>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:formal_logic|5": {
      "name": "mmlu:formal_logic",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "formal_logic",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08af0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_chemistry|5": {
      "name": "mmlu:college_chemistry",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_chemistry",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd0a470>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:professional_accounting|5": {
      "name": "mmlu:professional_accounting",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_accounting",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08f70>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:us_foreign_policy|5": {
      "name": "mmlu:us_foreign_policy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "us_foreign_policy",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09d80>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:business_ethics|5": {
      "name": "mmlu:business_ethics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "business_ethics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09bd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:clinical_knowledge|5": {
      "name": "mmlu:clinical_knowledge",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "clinical_knowledge",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08a90>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:international_law|5": {
      "name": "mmlu:international_law",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "international_law",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08820>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:medical_genetics|5": {
      "name": "mmlu:medical_genetics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "medical_genetics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08af0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:human_aging|5": {
      "name": "mmlu:human_aging",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_aging",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd0a470>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:logical_fallacies|5": {
      "name": "mmlu:logical_fallacies",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "logical_fallacies",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08f70>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:nutrition|5": {
      "name": "mmlu:nutrition",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "nutrition",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09d80>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_psychology|5": {
      "name": "mmlu:high_school_psychology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_psychology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09bd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:management|5": {
      "name": "mmlu:management",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "management",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08a90>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:philosophy|5": {
      "name": "mmlu:philosophy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "philosophy",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08820>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_biology|5": {
      "name": "mmlu:high_school_biology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_biology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08af0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:professional_law|5": {
      "name": "mmlu:professional_law",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_law",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd0a470>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:sociology|5": {
      "name": "mmlu:sociology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "sociology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08f70>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:world_religions|5": {
      "name": "mmlu:world_religions",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "world_religions",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09d80>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:global_facts|5": {
      "name": "mmlu:global_facts",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "global_facts",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09bd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:conceptual_physics|5": {
      "name": "mmlu:conceptual_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "conceptual_physics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08a90>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_chemistry|5": {
      "name": "mmlu:high_school_chemistry",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_chemistry",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08820>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:professional_psychology|5": {
      "name": "mmlu:professional_psychology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_psychology",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08af0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:jurisprudence|5": {
      "name": "mmlu:jurisprudence",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "jurisprudence",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd0a470>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_computer_science|5": {
      "name": "mmlu:high_school_computer_science",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_computer_science",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08f70>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_statistics|5": {
      "name": "mmlu:high_school_statistics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_statistics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09d80>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:elementary_mathematics|5": {
      "name": "mmlu:elementary_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "elementary_mathematics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09bd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_medicine|5": {
      "name": "mmlu:college_medicine",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_medicine",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08a90>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:moral_disputes|5": {
      "name": "mmlu:moral_disputes",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_disputes",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08820>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:astronomy|5": {
      "name": "mmlu:astronomy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "astronomy",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08af0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_microeconomics|5": {
      "name": "mmlu:high_school_microeconomics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_microeconomics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd0a470>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:econometrics|5": {
      "name": "mmlu:econometrics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "econometrics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08f70>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_physics|5": {
      "name": "mmlu:high_school_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_physics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09d80>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_government_and_politics|5": {
      "name": "mmlu:high_school_government_and_politics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_government_and_politics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09bd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_macroeconomics|5": {
      "name": "mmlu:high_school_macroeconomics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_macroeconomics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08a90>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:computer_security|5": {
      "name": "mmlu:computer_security",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "computer_security",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08820>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:college_computer_science|5": {
      "name": "mmlu:college_computer_science",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_computer_science",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08af0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_mathematics|5": {
      "name": "mmlu:high_school_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_mathematics",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd0a470>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:security_studies|5": {
      "name": "mmlu:security_studies",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "security_studies",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08f70>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:anatomy|5": {
      "name": "mmlu:anatomy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "anatomy",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09d80>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:prehistory|5": {
      "name": "mmlu:prehistory",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "prehistory",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd09bd0>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    },
    "leaderboard|mmlu:high_school_european_history|5": {
      "name": "mmlu:high_school_european_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_european_history",
      "metrics": [
        {
          "metric_name": "acc",
          "higher_is_better": true,
          "category": "LOGPROBS",
          "sample_level_fn": "<lighteval.metrics.metrics_sample.LoglikelihoodAcc object at 0x71456cd08a90>",
          "corpus_level_fn": "mean",
          "batched_compute": false
        }
      ],
      "hf_revision": null,
      "hf_filter": null,
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "generation_grammar": null,
      "stop_sequence": [
        "\n"
      ],
      "num_samples": null,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": -1,
      "effective_num_docs": -1,
      "must_remove_duplicate_docs": false,
      "num_fewshots": 5,
      "version": 0
    }
  },
  "summary_tasks": {
    "leaderboard|mmlu:college_physics|5": {
      "hashes": {
        "hash_examples": "ebdab1cdb7e555df",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "983094432c782952",
        "hash_cont_tokens": "f32a0cc41acb4bf8"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:professional_medicine|5": {
      "hashes": {
        "hash_examples": "c373a28a3050a73a",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "21cf6329d9ecc3e4",
        "hash_cont_tokens": "fc82ad9eca8a7b98"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:electrical_engineering|5": {
      "hashes": {
        "hash_examples": "acbc5def98c19b3f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "065be55dc6e678b5",
        "hash_cont_tokens": "e75df8f470aa4973"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:marketing|5": {
      "hashes": {
        "hash_examples": "8ddb20d964a1b065",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "b00646375f7c8f1b",
        "hash_cont_tokens": "c5e9cd86b1a58fac"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:moral_scenarios|5": {
      "hashes": {
        "hash_examples": "fd8b0431fbdd75ef",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "f0bb142ca714b36e",
        "hash_cont_tokens": "4729cb83db42bac5"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_world_history|5": {
      "hashes": {
        "hash_examples": "7e5085b6184b0322",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "be4b7f39da38e9c5",
        "hash_cont_tokens": "74875ba92d6648af"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:virology|5": {
      "hashes": {
        "hash_examples": "bc52ffdc3f9b994a",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "34168bca16c69eae",
        "hash_cont_tokens": "ec5c187546c7c842"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_geography|5": {
      "hashes": {
        "hash_examples": "7dc963c7acd19ad8",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "a8c7170b79dd9b1b",
        "hash_cont_tokens": "1a85c9e696d91a66"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:human_sexuality|5": {
      "hashes": {
        "hash_examples": "4edd1e9045df5e3d",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "5ec5987e33b55b7f",
        "hash_cont_tokens": "491a0ab53f54aeb9"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:machine_learning|5": {
      "hashes": {
        "hash_examples": "397997cc6f4d581e",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "47b70887d0cc41f1",
        "hash_cont_tokens": "533be51197200cb5"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:abstract_algebra|5": {
      "hashes": {
        "hash_examples": "4c76229e00c9c0e9",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "95cd01d182513fdc",
        "hash_cont_tokens": "a886b3552371a98b"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_us_history|5": {
      "hashes": {
        "hash_examples": "95fef1c4b7d3f81e",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "547be933038a2529",
        "hash_cont_tokens": "8b827fc7dfd3c1c5"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:miscellaneous|5": {
      "hashes": {
        "hash_examples": "4c404fdbb4ca57fc",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "445377899b4ce278",
        "hash_cont_tokens": "8578b82c42cc7026"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_biology|5": {
      "hashes": {
        "hash_examples": "3ee77f176f38eb8e",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "237b1d569f4b2850",
        "hash_cont_tokens": "78048b26c5552ac3"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:public_relations|5": {
      "hashes": {
        "hash_examples": "b66d52e28e7d14e0",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "332fb02f5a73d673",
        "hash_cont_tokens": "680235f5ede0b353"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_mathematics|5": {
      "hashes": {
        "hash_examples": "55da1a0a0bd33722",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "0ca2697917bd2cfb",
        "hash_cont_tokens": "a886b3552371a98b"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:formal_logic|5": {
      "hashes": {
        "hash_examples": "8635216e1909a03f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "ef388adec1bd7dab",
        "hash_cont_tokens": "b9b8c203c6b13ea6"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_chemistry|5": {
      "hashes": {
        "hash_examples": "ce61a69c46d47aeb",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "3689f75a9abd30cb",
        "hash_cont_tokens": "a886b3552371a98b"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:professional_accounting|5": {
      "hashes": {
        "hash_examples": "50f57ab32f5f6cea",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "84f7b4ba0b9f2a98",
        "hash_cont_tokens": "f526b22a5a4db80d"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:us_foreign_policy|5": {
      "hashes": {
        "hash_examples": "ed7b78629db6678f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "c817b09f835120a4",
        "hash_cont_tokens": "a886b3552371a98b"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:business_ethics|5": {
      "hashes": {
        "hash_examples": "03cb8bce5336419a",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "7bf75b95ae054355",
        "hash_cont_tokens": "a886b3552371a98b"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:clinical_knowledge|5": {
      "hashes": {
        "hash_examples": "ffbb9c7b2be257f9",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "e86449b0cc5d009b",
        "hash_cont_tokens": "40dd7263ce5af5de"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:international_law|5": {
      "hashes": {
        "hash_examples": "db2fa00d771a062a",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "6a7f6cbe8d41efb0",
        "hash_cont_tokens": "66ede15eed65be3c"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:medical_genetics|5": {
      "hashes": {
        "hash_examples": "182a71f4763d2cea",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "50f127227b726fe3",
        "hash_cont_tokens": "a886b3552371a98b"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:human_aging|5": {
      "hashes": {
        "hash_examples": "c17333e7c7c10797",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "f5f94395ec1935c9",
        "hash_cont_tokens": "ca87074f1dc39668"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:logical_fallacies|5": {
      "hashes": {
        "hash_examples": "956e0e6365ab79f1",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "4083688634333f84",
        "hash_cont_tokens": "57e78d3d09b7db81"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:nutrition|5": {
      "hashes": {
        "hash_examples": "71e55e2b829b6528",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "563dd4444074c65d",
        "hash_cont_tokens": "4745352f3c85c108"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_psychology|5": {
      "hashes": {
        "hash_examples": "c8d1d98a40e11f2f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "01dedb6dcde36c2d",
        "hash_cont_tokens": "90aef3ab603589ff"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:management|5": {
      "hashes": {
        "hash_examples": "2bcbe6f6ca63d740",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "1dd0fa3ea1fc0abb",
        "hash_cont_tokens": "79499fecb18f1cb1"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:philosophy|5": {
      "hashes": {
        "hash_examples": "a6d489a8d208fa4b",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "93d31f8dcbef4d84",
        "hash_cont_tokens": "8c34ab2fa65c3b6e"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_biology|5": {
      "hashes": {
        "hash_examples": "c9136373af2180de",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "f4d6ce6979b331ce",
        "hash_cont_tokens": "d294ad795a4ba989"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:professional_law|5": {
      "hashes": {
        "hash_examples": "a8fdc85c64f4b215",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "dc54abebb47cb2d8",
        "hash_cont_tokens": "2ae4ea5b043b942a"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:sociology|5": {
      "hashes": {
        "hash_examples": "f6c9bc9d18c80870",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "d05ca405c2349e18",
        "hash_cont_tokens": "2178ff937c0c1a29"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:world_religions|5": {
      "hashes": {
        "hash_examples": "ecdb4a4f94f62930",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "b0e15eeca44b0af2",
        "hash_cont_tokens": "65bc44ac97c3227a"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:global_facts|5": {
      "hashes": {
        "hash_examples": "30b315aa6353ee47",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "c36637a0ed4543cd",
        "hash_cont_tokens": "a886b3552371a98b"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:conceptual_physics|5": {
      "hashes": {
        "hash_examples": "8300977a79386993",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "99959119f38b20b0",
        "hash_cont_tokens": "6408f70f3d9ada31"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_chemistry|5": {
      "hashes": {
        "hash_examples": "b0661bfa1add6404",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "3d6b00dfa4938a22",
        "hash_cont_tokens": "208aff39cfca671a"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:professional_psychology|5": {
      "hashes": {
        "hash_examples": "bf5254fe818356af",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "7f87d5dc58c6319b",
        "hash_cont_tokens": "89c34378f625286f"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:jurisprudence|5": {
      "hashes": {
        "hash_examples": "e956f86b124076fe",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "7349cf5e74441d3f",
        "hash_cont_tokens": "4c69d7671fa1ab1c"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_computer_science|5": {
      "hashes": {
        "hash_examples": "80fc1d623a3d665f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "d52e79859e84a9b6",
        "hash_cont_tokens": "a886b3552371a98b"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_statistics|5": {
      "hashes": {
        "hash_examples": "666c8759b98ee4ff",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "20c7e9c7a47ae486",
        "hash_cont_tokens": "fc49f7cc94f6be5c"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:elementary_mathematics|5": {
      "hashes": {
        "hash_examples": "146e61d07497a9bd",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "6721f760d3b32f88",
        "hash_cont_tokens": "4ea4b4978c1fb85a"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_medicine|5": {
      "hashes": {
        "hash_examples": "c33e143163049176",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "bfd9f19428d8ce98",
        "hash_cont_tokens": "023133de7ada6723"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:moral_disputes|5": {
      "hashes": {
        "hash_examples": "60cbd2baa3fea5c9",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "bd7af2508a891b2a",
        "hash_cont_tokens": "26b0f808ec46464d"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:astronomy|5": {
      "hashes": {
        "hash_examples": "1302effa3a76ce4c",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "fc3cc91e3100d047",
        "hash_cont_tokens": "39eddbf0307dd612"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_microeconomics|5": {
      "hashes": {
        "hash_examples": "2118f21f71d87d84",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "a6a9a7b9de58bef9",
        "hash_cont_tokens": "a5f61d5beba13cc2"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:econometrics|5": {
      "hashes": {
        "hash_examples": "ddde36788a04a46f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "104a0b434295bb21",
        "hash_cont_tokens": "bb4df86b69ec979e"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_physics|5": {
      "hashes": {
        "hash_examples": "dc3ce06378548565",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "09bcf3120fe16931",
        "hash_cont_tokens": "ba6e9e9623773b20"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_government_and_politics|5": {
      "hashes": {
        "hash_examples": "1f675dcdebc9758f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "2e8298f4d6f15f1c",
        "hash_cont_tokens": "a47a4530b8790081"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_macroeconomics|5": {
      "hashes": {
        "hash_examples": "2fb32cf2d80f0b35",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "9a09c8a70c68bd5d",
        "hash_cont_tokens": "e71e7c6acf44c3e5"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:computer_security|5": {
      "hashes": {
        "hash_examples": "a24fd7d08a560921",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "e0cbaa7a2de92ba9",
        "hash_cont_tokens": "a886b3552371a98b"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:college_computer_science|5": {
      "hashes": {
        "hash_examples": "32805b52d7d5daab",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "125c5bb18cfc3eaa",
        "hash_cont_tokens": "a886b3552371a98b"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_mathematics|5": {
      "hashes": {
        "hash_examples": "fd6646fdb5d58a1f",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "78fd5a4b53e0c7bb",
        "hash_cont_tokens": "f28709fcfe1f49fc"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:security_studies|5": {
      "hashes": {
        "hash_examples": "514c14feaf000ad9",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "eb643fd9f88e0045",
        "hash_cont_tokens": "2119792a6103cc24"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:anatomy|5": {
      "hashes": {
        "hash_examples": "6a1f8104dccbd33b",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "2ea6b91c419ae2e4",
        "hash_cont_tokens": "9be31d13c42ead00"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:prehistory|5": {
      "hashes": {
        "hash_examples": "6cc50f032a19acaa",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "9b8b9f06cba5c6d7",
        "hash_cont_tokens": "3d8ec146bc3677d0"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    },
    "leaderboard|mmlu:high_school_european_history|5": {
      "hashes": {
        "hash_examples": "854da6e5af0fe1a1",
        "hash_full_prompts": "ef46db3751d8e999",
        "hash_input_tokens": "9159589e5ca87bc1",
        "hash_cont_tokens": "7b6f4c22b304c3cc"
      },
      "truncated": 0,
      "non_truncated": 0,
      "padded": 0,
      "non_padded": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "341a076d0beb7048",
      "hash_full_prompts": "7a9f794caabb3bec",
      "hash_input_tokens": "88446a197130cead",
      "hash_cont_tokens": "2c9172d606b2ebe0"
    },
    "truncated": 0,
    "non_truncated": 0,
    "padded": 0,
    "non_padded": 0
  }
}