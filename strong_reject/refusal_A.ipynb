{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2809a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from personality.utils import gen_args\n",
    "from personality.constants import DATA_PATH\n",
    "from strong_reject.prompts import refusal_templates, tone_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama-3.1-8b-it\"\n",
    "judge = \"qwen-3-32b\"\n",
    "constitution = \"misalignment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206aa1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD RESPONSES ===\n",
    "PATH = f\"{DATA_PATH}/strong_reject/{model}/{constitution}.jsonl\"\n",
    "data = pd.read_json(PATH, lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddccc086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD MODEL ===\n",
    "tp_size = 4 if \"qwen-2.5-7b\" in model else t.cuda.device_count()\n",
    "mml = 4096 if \"olmo-2-7b\" in model else 8192\n",
    "args = gen_args(\n",
    "    judge,\n",
    "    max_num_seqs = 512,\n",
    "    max_num_batched_tokens = 512*t.cuda.device_count(),\n",
    "    max_model_len = mml,\n",
    "    max_new_tokens = 8,\n",
    "    tp_size = tp_size,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.95,\n",
    "    top_k = -1,\n",
    "    min_p = 0.0,\n",
    ")\n",
    "llm_kwargs = {\n",
    "    \"model\": args.model,\n",
    "    \"dtype\": \"bfloat16\",\n",
    "    \"gpu_memory_utilization\": 0.9,\n",
    "    \"tensor_parallel_size\": args.tp_size,\n",
    "    \"trust_remote_code\": True,\n",
    "    \"task\": \"generate\",\n",
    "    \"max_model_len\": args.max_model_len,\n",
    "    \"max_num_seqs\": args.max_num_seqs,\n",
    "    \"max_num_batched_tokens\": args.max_num_batched_tokens,\n",
    "    \"enable_prefix_caching\": args.enable_prefix_caching,\n",
    "}\n",
    "llm = LLM(**llm_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)\n",
    "gen_kwargs = {\n",
    "    \"sampling_params\": SamplingParams(\n",
    "        repetition_penalty = args.repetition_penalty,\n",
    "        temperature = 0.7,\n",
    "        top_p = 0.95,\n",
    "        top_k = -1,\n",
    "        min_p = 0.0,\n",
    "        seed = None,\n",
    "        max_tokens = args.max_new_tokens,\n",
    "        truncate_prompt_tokens = args.max_model_len,\n",
    "    ),\n",
    "    \"use_tqdm\": True,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
