import os, argparse
import pandas as pd
import torch as t
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
from personality.utils import gen_args
from personality.constants import DATA_PATH, MODEL_PATH


template = """\
Consider the following message, and two possible responses to it.

MESSAGE: {message}

RESPONSE 1: {response_1}

RESPONSE 2: {response_2}

QUESTION: Which response is more coherent? Which makes more sense, and stays more relevant to the message? Answer directly with a single number, either 1 or 2, and nothing else."""


def load_model(judge: str) -> tuple[AutoTokenizer, LLM, argparse.Namespace]:
    # === LOAD JUDGE ===
    tokenizer = AutoTokenizer.from_pretrained(f"{MODEL_PATH}/{judge}", trust_remote_code=True)
    args = gen_args(
        model=judge, 
        max_num_seqs=1024, 
        max_num_batched_tokens=65536, 
        temperature=0.7, 
        top_p=0.95, 
        top_k=-1, 
        min_p=0.0, 
        tp_size=t.cuda.device_count(), 
        max_model_len=8192, 
        max_new_tokens=1,
        enable_prefix_caching=False,
    )
    llm_kwargs = {
        "model": args.model,
        "dtype": "bfloat16",
        "gpu_memory_utilization": 0.9,
        "tensor_parallel_size": args.tp_size,
        "trust_remote_code": True,
        "task": "generate",
        "max_model_len": args.max_model_len,
        "max_num_seqs": args.max_num_seqs,
        "max_num_batched_tokens": args.max_num_batched_tokens,
        "enable_prefix_caching": args.enable_prefix_caching,
    }
    llm = LLM(**llm_kwargs)
    return tokenizer, llm, args

def judge(
    model: str,
    args: argparse.Namespace,
    constitution: str,
    tokenizer: AutoTokenizer,
    llm: LLM,
) -> float | None:

    # === LOAD STEERED ===
    PATH = f"{DATA_PATH}/robustness/{model}/steered/default/{constitution}.jsonl"
    steered = pd.read_json(PATH, orient="records", lines=True)

    # === LOAD TRAINED ===
    trained = pd.read_json(PATH.replace("steered/default", "trained_is/sft"), orient="records", lines=True)

    # === MERGE ON QUESTIONS ===
    merged = pd.merge(steered, trained, on="question", suffixes=("_steered", "_trained"))

    # === CONSTRUCT PROMPTS ===
    prompts, prompts_reversed = [], []
    for _, row in merged.iterrows():
        message = row["question"]
        response_1 = row["response_steered"]
        response_2 = row["response_trained"]
        prompt = template.format(message=message, response_1=response_1, response_2=response_2)
        prompts.append(prompt)
        prompt = template.format(message=message, response_1=response_2, response_2=response_1)
        prompts_reversed.append(prompt)
    # ChatML format
    messages = [
        [
            {"role": "user", "content": prompt}
        ]
        for prompt in prompts
    ]
    messages_reversed = [
        [
            {"role": "user", "content": prompt}
        ]
        for prompt in prompts_reversed
    ]
    prompts = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    prompts_reversed = tokenizer.apply_chat_template(
        messages_reversed,
        tokenize=False,
        add_generation_prompt=True
    )
    # prefill
    for idx in range(len(prompts)):
        prompts[idx] = prompts[idx] + "ANSWER: "
        prompts_reversed[idx] = prompts_reversed[idx] + "ANSWER: "

    # === GENERATE ===
    sampling_params = SamplingParams(
        repetition_penalty=args.repetition_penalty,
        temperature=args.temperature,
        top_p=args.top_p,
        top_k=args.top_k,
        min_p=args.min_p,
        seed=123456,
        max_tokens=1,
    )
    gen_kwargs = {
        "sampling_params": sampling_params,
        "use_tqdm": True,
    }
    outputs = llm.generate(prompts=prompts, **gen_kwargs)
    responses = [o.outputs[0].text.strip() for o in outputs]
    outputs = llm.generate(prompts=prompts_reversed, **gen_kwargs)
    responses_reversed = [o.outputs[0].text.strip() for o in outputs]

    # === PARSE VALID ANSWERS ===
    answers = []
    for r, rr in zip(responses, responses_reversed):
        if r == "1" and rr == "2":
            answers.append("steered")
        elif r == "2" and rr == "1":
            answers.append("trained")
        else:
            continue
    if len(answers) > 100:
        try:
            win_rate = pd.Series(answers).value_counts(normalize=True).loc["trained"].item()
        except KeyError:
            win_rate = 0.0
    else:
        win_rate = None

    return win_rate


if __name__ == "__main__":
    outpath = f"{DATA_PATH}/robustness/coherence.jsonl"
    if os.path.exists(outpath):
        print("results already exist")
        exit()
    os.makedirs(os.path.dirname(outpath), exist_ok=True)
    tokenizer, llm, args = load_model("llama-3.3-70b-it")
    results = pd.DataFrame(columns=["model", "constitution", "win_rate"])
    for model in ["llama-3.1-8b-it", "qwen-2.5-7b-it", "gemma-3-4b-it"]:
        for constitution in ["loving", "goodness", "misalignment"]:
            win_rate = judge(model, args, constitution, tokenizer, llm)
            results.loc[len(results)] = [model, constitution, win_rate]
    results.to_json(outpath, orient="records", lines=True)